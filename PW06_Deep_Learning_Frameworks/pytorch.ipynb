{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 MLP with PyTorch for CIFAR10\n",
    "\n",
    "Based on the notebooks provided and discussed during the lecture, set up a notebook for the configuration and training of a MLP on the CIFAR10 images.\n",
    "\n",
    "[These images](https://www.cs.toronto.edu/~kriz/cifar.html) of size 32x32 pixel show 10 different object categories\n",
    "\n",
    "Your implementation should provide the following features : \n",
    "\n",
    "- Download of the CIFAR10 images form an appropriate server and local storage for further usage.\n",
    "- Configuration of a custom Dataset (c.f. chapter 6.2.2 in the lecture notes) that will provide the CIFAR10 data to the DataLoader class during training.\n",
    "- Set up of a SummaryWriter for TensorBoard (c.f. chapter 6.2.3 in the lecture notes) and output of a set of CIFAR10 images (as grid like the figures above) to TensorBoard.\n",
    "- Set up of a MLP with confiigurable number of hidden layers that takes CIFAR10 (colour) images as input.\n",
    "- Preparation of the SummaryWriter for continuous output of training and validation loss/error to TensorBoard during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~ Imports ~~~\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets.cifar import CIFAR10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# ~~~ Transforms ~~~\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~ Dataset ~~~\n",
    "class CustomCIFAR10Dataset(CIFAR10):\n",
    "    def __init__(self, **kwargs):\n",
    "        cifar10_dataset = CIFAR10(**kwargs)\n",
    "        self.cifar10_dataset = cifar10_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifar10_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.cifar10_dataset[idx]\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# ~~~ Download CIFAR10 Dataset ~~~\n",
    "train_dataset = CustomCIFAR10Dataset(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CustomCIFAR10Dataset(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~ Parameters ~~~\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "num_hidden_layers = 2\n",
    "hidden_layer_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~ Data loaders ~~~\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~ MLP ~~~\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_hidden_layers, hidden_layer_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [nn.Flatten()]\n",
    "        layers.append(nn.Linear(input_size, hidden_layer_size))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_layer_size, hidden_layer_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(hidden_layer_size, num_classes))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~ Experience ~~~\n",
    "model = MLP(3 * 32 * 32, 10, num_hidden_layers, hidden_layer_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1563], Loss: 1.8642\n",
      "Epoch [1/10], Step [200/1563], Loss: 1.6989\n",
      "Epoch [1/10], Step [300/1563], Loss: 1.5546\n",
      "Epoch [1/10], Step [400/1563], Loss: 1.4690\n",
      "Epoch [1/10], Step [500/1563], Loss: 1.5200\n",
      "Epoch [1/10], Step [600/1563], Loss: 1.3329\n",
      "Epoch [1/10], Step [700/1563], Loss: 1.4116\n",
      "Epoch [1/10], Step [800/1563], Loss: 1.5245\n",
      "Epoch [1/10], Step [900/1563], Loss: 1.4949\n",
      "Epoch [1/10], Step [1000/1563], Loss: 1.8159\n",
      "Epoch [1/10], Step [1100/1563], Loss: 1.3743\n",
      "Epoch [1/10], Step [1200/1563], Loss: 1.5110\n",
      "Epoch [1/10], Step [1300/1563], Loss: 1.4013\n",
      "Epoch [1/10], Step [1400/1563], Loss: 1.2074\n",
      "Epoch [1/10], Step [1500/1563], Loss: 1.7704\n",
      "Epoch [2/10], Step [100/1563], Loss: 1.4996\n",
      "Epoch [2/10], Step [200/1563], Loss: 1.1926\n",
      "Epoch [2/10], Step [300/1563], Loss: 1.5976\n",
      "Epoch [2/10], Step [400/1563], Loss: 1.2378\n",
      "Epoch [2/10], Step [500/1563], Loss: 1.3722\n",
      "Epoch [2/10], Step [600/1563], Loss: 1.2517\n",
      "Epoch [2/10], Step [700/1563], Loss: 1.4453\n",
      "Epoch [2/10], Step [800/1563], Loss: 1.1839\n",
      "Epoch [2/10], Step [900/1563], Loss: 1.4447\n",
      "Epoch [2/10], Step [1000/1563], Loss: 1.8356\n",
      "Epoch [2/10], Step [1100/1563], Loss: 1.4147\n",
      "Epoch [2/10], Step [1200/1563], Loss: 1.6783\n",
      "Epoch [2/10], Step [1300/1563], Loss: 1.4313\n",
      "Epoch [2/10], Step [1400/1563], Loss: 1.2600\n",
      "Epoch [2/10], Step [1500/1563], Loss: 1.1303\n",
      "Epoch [3/10], Step [100/1563], Loss: 1.7220\n",
      "Epoch [3/10], Step [200/1563], Loss: 1.1984\n",
      "Epoch [3/10], Step [300/1563], Loss: 1.0807\n",
      "Epoch [3/10], Step [400/1563], Loss: 1.4808\n",
      "Epoch [3/10], Step [500/1563], Loss: 1.3892\n",
      "Epoch [3/10], Step [600/1563], Loss: 1.4054\n",
      "Epoch [3/10], Step [700/1563], Loss: 1.7304\n",
      "Epoch [3/10], Step [800/1563], Loss: 1.1419\n",
      "Epoch [3/10], Step [900/1563], Loss: 1.2711\n",
      "Epoch [3/10], Step [1000/1563], Loss: 1.2990\n",
      "Epoch [3/10], Step [1100/1563], Loss: 1.5880\n",
      "Epoch [3/10], Step [1200/1563], Loss: 1.1352\n",
      "Epoch [3/10], Step [1300/1563], Loss: 1.2711\n",
      "Epoch [3/10], Step [1400/1563], Loss: 1.0939\n",
      "Epoch [3/10], Step [1500/1563], Loss: 1.5290\n",
      "Epoch [4/10], Step [100/1563], Loss: 1.3987\n",
      "Epoch [4/10], Step [200/1563], Loss: 1.1497\n",
      "Epoch [4/10], Step [300/1563], Loss: 1.0036\n",
      "Epoch [4/10], Step [400/1563], Loss: 0.9081\n",
      "Epoch [4/10], Step [500/1563], Loss: 1.3078\n",
      "Epoch [4/10], Step [600/1563], Loss: 1.0612\n",
      "Epoch [4/10], Step [700/1563], Loss: 1.1913\n",
      "Epoch [4/10], Step [800/1563], Loss: 1.2375\n",
      "Epoch [4/10], Step [900/1563], Loss: 0.8726\n",
      "Epoch [4/10], Step [1000/1563], Loss: 1.4056\n",
      "Epoch [4/10], Step [1100/1563], Loss: 1.3894\n",
      "Epoch [4/10], Step [1200/1563], Loss: 1.1097\n",
      "Epoch [4/10], Step [1300/1563], Loss: 1.6331\n",
      "Epoch [4/10], Step [1400/1563], Loss: 1.5766\n",
      "Epoch [4/10], Step [1500/1563], Loss: 1.2514\n",
      "Epoch [5/10], Step [100/1563], Loss: 1.2209\n",
      "Epoch [5/10], Step [200/1563], Loss: 0.9493\n",
      "Epoch [5/10], Step [300/1563], Loss: 1.1953\n",
      "Epoch [5/10], Step [400/1563], Loss: 1.0324\n",
      "Epoch [5/10], Step [500/1563], Loss: 1.3122\n",
      "Epoch [5/10], Step [600/1563], Loss: 1.4566\n",
      "Epoch [5/10], Step [700/1563], Loss: 1.4028\n",
      "Epoch [5/10], Step [800/1563], Loss: 1.2754\n",
      "Epoch [5/10], Step [900/1563], Loss: 1.3507\n",
      "Epoch [5/10], Step [1000/1563], Loss: 1.2906\n",
      "Epoch [5/10], Step [1100/1563], Loss: 1.2554\n",
      "Epoch [5/10], Step [1200/1563], Loss: 1.0353\n",
      "Epoch [5/10], Step [1300/1563], Loss: 1.2028\n",
      "Epoch [5/10], Step [1400/1563], Loss: 1.2892\n",
      "Epoch [5/10], Step [1500/1563], Loss: 1.0013\n",
      "Epoch [6/10], Step [100/1563], Loss: 0.8195\n",
      "Epoch [6/10], Step [200/1563], Loss: 0.7315\n",
      "Epoch [6/10], Step [300/1563], Loss: 1.1514\n",
      "Epoch [6/10], Step [400/1563], Loss: 1.0457\n",
      "Epoch [6/10], Step [500/1563], Loss: 1.3184\n",
      "Epoch [6/10], Step [600/1563], Loss: 1.4227\n",
      "Epoch [6/10], Step [700/1563], Loss: 1.0776\n",
      "Epoch [6/10], Step [800/1563], Loss: 1.1493\n",
      "Epoch [6/10], Step [900/1563], Loss: 1.1118\n",
      "Epoch [6/10], Step [1000/1563], Loss: 1.0215\n",
      "Epoch [6/10], Step [1100/1563], Loss: 1.3492\n",
      "Epoch [6/10], Step [1200/1563], Loss: 1.1892\n",
      "Epoch [6/10], Step [1300/1563], Loss: 1.3209\n",
      "Epoch [6/10], Step [1400/1563], Loss: 1.2363\n",
      "Epoch [6/10], Step [1500/1563], Loss: 1.1542\n",
      "Epoch [7/10], Step [100/1563], Loss: 0.9792\n",
      "Epoch [7/10], Step [200/1563], Loss: 0.9684\n",
      "Epoch [7/10], Step [300/1563], Loss: 1.2129\n",
      "Epoch [7/10], Step [400/1563], Loss: 1.3260\n",
      "Epoch [7/10], Step [500/1563], Loss: 1.1119\n",
      "Epoch [7/10], Step [600/1563], Loss: 1.2698\n",
      "Epoch [7/10], Step [700/1563], Loss: 1.4207\n",
      "Epoch [7/10], Step [800/1563], Loss: 1.1636\n",
      "Epoch [7/10], Step [900/1563], Loss: 0.8517\n",
      "Epoch [7/10], Step [1000/1563], Loss: 1.0249\n",
      "Epoch [7/10], Step [1100/1563], Loss: 1.0577\n",
      "Epoch [7/10], Step [1200/1563], Loss: 0.8711\n",
      "Epoch [7/10], Step [1300/1563], Loss: 1.4180\n",
      "Epoch [7/10], Step [1400/1563], Loss: 1.4412\n",
      "Epoch [7/10], Step [1500/1563], Loss: 1.4634\n",
      "Epoch [8/10], Step [100/1563], Loss: 1.1101\n",
      "Epoch [8/10], Step [200/1563], Loss: 0.8325\n",
      "Epoch [8/10], Step [300/1563], Loss: 1.1319\n",
      "Epoch [8/10], Step [400/1563], Loss: 0.6193\n",
      "Epoch [8/10], Step [500/1563], Loss: 0.8045\n",
      "Epoch [8/10], Step [600/1563], Loss: 1.0622\n",
      "Epoch [8/10], Step [700/1563], Loss: 0.8947\n",
      "Epoch [8/10], Step [800/1563], Loss: 1.5549\n",
      "Epoch [8/10], Step [900/1563], Loss: 0.8112\n",
      "Epoch [8/10], Step [1000/1563], Loss: 1.2103\n",
      "Epoch [8/10], Step [1100/1563], Loss: 1.1249\n",
      "Epoch [8/10], Step [1200/1563], Loss: 0.9556\n",
      "Epoch [8/10], Step [1300/1563], Loss: 1.2014\n",
      "Epoch [8/10], Step [1400/1563], Loss: 1.1841\n",
      "Epoch [8/10], Step [1500/1563], Loss: 0.8787\n",
      "Epoch [9/10], Step [100/1563], Loss: 0.9345\n",
      "Epoch [9/10], Step [200/1563], Loss: 1.2756\n",
      "Epoch [9/10], Step [300/1563], Loss: 1.3939\n",
      "Epoch [9/10], Step [400/1563], Loss: 0.9245\n",
      "Epoch [9/10], Step [500/1563], Loss: 1.1217\n",
      "Epoch [9/10], Step [600/1563], Loss: 0.9388\n",
      "Epoch [9/10], Step [700/1563], Loss: 1.6154\n",
      "Epoch [9/10], Step [800/1563], Loss: 1.0219\n",
      "Epoch [9/10], Step [900/1563], Loss: 0.9251\n",
      "Epoch [9/10], Step [1000/1563], Loss: 0.7952\n",
      "Epoch [9/10], Step [1100/1563], Loss: 0.7826\n",
      "Epoch [9/10], Step [1200/1563], Loss: 0.9536\n",
      "Epoch [9/10], Step [1300/1563], Loss: 0.9128\n",
      "Epoch [9/10], Step [1400/1563], Loss: 0.9773\n",
      "Epoch [9/10], Step [1500/1563], Loss: 1.2518\n",
      "Epoch [10/10], Step [100/1563], Loss: 0.7871\n",
      "Epoch [10/10], Step [200/1563], Loss: 1.1566\n",
      "Epoch [10/10], Step [300/1563], Loss: 1.3235\n",
      "Epoch [10/10], Step [400/1563], Loss: 0.8116\n",
      "Epoch [10/10], Step [500/1563], Loss: 1.0752\n",
      "Epoch [10/10], Step [600/1563], Loss: 0.8679\n",
      "Epoch [10/10], Step [700/1563], Loss: 0.8175\n",
      "Epoch [10/10], Step [800/1563], Loss: 0.7258\n",
      "Epoch [10/10], Step [900/1563], Loss: 0.8615\n",
      "Epoch [10/10], Step [1000/1563], Loss: 0.9702\n",
      "Epoch [10/10], Step [1100/1563], Loss: 0.8001\n",
      "Epoch [10/10], Step [1200/1563], Loss: 0.9505\n",
      "Epoch [10/10], Step [1300/1563], Loss: 0.9954\n",
      "Epoch [10/10], Step [1400/1563], Loss: 0.9764\n",
      "Epoch [10/10], Step [1500/1563], Loss: 1.0193\n"
     ]
    }
   ],
   "source": [
    "# ~~~ TensorBoard Setup ~~~\n",
    "writer = SummaryWriter('runs/cifar10_experiment')\n",
    "\n",
    "# ~~~ Training ~~~\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Log training information to TensorBoard\n",
    "    writer.add_scalar('training loss', loss.item(), epoch)\n",
    "\n",
    "    # Log images to TensorBoard\n",
    "    img_grid = make_grid(images)\n",
    "    writer.add_image('CIFAR10 Images', img_grid, epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
